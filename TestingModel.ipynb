{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Saved Model"
      ],
      "metadata": {
        "id": "yvyUit9iJAfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "taPhRPmH6YdY"
      },
      "outputs": [],
      "source": [
        "# --- Install requirements if not yet done ---\n",
        "!pip install -r /content/jawirumi_v1_model/requirements.txt --quiet\n",
        "\n",
        "# --- Import after installation ---\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# --- Load once (already trained + saved) ---\n",
        "def load_model(model_path=\"lora_model\", max_seq_length=2048, dtype=None, load_in_4bit=True):\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_path,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=dtype,\n",
        "        load_in_4bit=load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = load_model(\"/content/jawirumi_v1_model\")"
      ],
      "metadata": {
        "id": "RpQek182I5-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: Jawi → Rumi"
      ],
      "metadata": {
        "id": "vXlZeK36JLJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "\n",
        "def jawi2rumi_generate(model, tokenizer, prompt: str,\n",
        "                       chat_template=\"llama-3.1\",\n",
        "                       max_new_tokens=128,\n",
        "                       temperature=0.1,\n",
        "                       min_p=0.1):\n",
        "    \"\"\"\n",
        "    Generate transliteration or text output using an Unsloth fine-tuned model.\n",
        "\n",
        "    Args:\n",
        "        model: Loaded FastLanguageModel\n",
        "        tokenizer: Corresponding tokenizer\n",
        "        prompt (str): User input or Jawi text to process\n",
        "        chat_template (str): Chat template used during training (default: llama-3.1)\n",
        "        max_new_tokens (int): Number of tokens to generate\n",
        "        temperature (float): Sampling temperature for creativity\n",
        "        min_p (float): Minimum probability threshold for nucleus sampling\n",
        "\n",
        "    Returns:\n",
        "        str: Generated output text\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure correct chat template\n",
        "    tokenizer = get_chat_template(tokenizer, chat_template=chat_template)\n",
        "\n",
        "    # Create text streamer for live streaming (optional)\n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    # Prepare input message\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"<jawi2rumi> {prompt}\"},\n",
        "    ]\n",
        "\n",
        "    # Tokenize and send to model device\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    # ✅ Create attention mask (all non-padding tokens = 1)\n",
        "    attention_mask = inputs.ne(tokenizer.pad_token_id).long()\n",
        "\n",
        "    # Run generation\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            attention_mask=attention_mask,\n",
        "            streamer=text_streamer,  # can remove if you don’t want live output\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            min_p=min_p,\n",
        "        )\n",
        "\n",
        "    # Decode final text\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated\n"
      ],
      "metadata": {
        "id": "2ccEQYCzJGuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = jawi2rumi_generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt=\"چينتا سجاتي اداله سوات ڤركارا يڠ مرجوع كڤد حقيقة باطن\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "b4l5fuwwJN-t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}